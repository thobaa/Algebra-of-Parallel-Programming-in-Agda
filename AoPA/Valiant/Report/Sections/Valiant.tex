\chapter{Valiant's Algorithm}
\label{Section:Valiant}
\label{Valiant}
In \cite{Valiant}, Leslie G. Valiant gave a divide and conquer algorithm for computing the transitive closure of an upper triangular matrix to prove that context free parsing has the same time complexity as matrix multiplication. The algorithm divides a string into two parts, parses them recursively, and then puts them together through a fairly complicated procedure that requires a constant number of matrix multiplications.

Since the algorithm is a divide and conquer algorithm (where the combine step is also fairly parallelizable), it could potentially be used for parsing in parallel, as suggested in \citep{JP-PP}.

\section{Derivation}
\label{Valiant:Der}
\label{Valiant:Derivation}
In this section, we are going to derive Valiant's algorithm from our specification \eqref{TC}.
\subsection{Main structure}
We want to compute the transitive closure of a parse chart. The main idea of the algorithm is to split the chart into two subcharts and a rectangular overlap region (containing parses of strings that overlap the corresponding splitting of the string). Next, compute the transitive closures of the subcharts, and combine them (somehow) to fill in the overlap part. A chart of size $1 \times 1$ contains just one element, which is zero because it is upper triangular, and the transitive closure of this is again the zero matrix.

\label{Section:Subdivision-in-Specification}
When the chart $X$ is $n \times n$, with $n > 1$, we can write it down as a block matrix 
\begin{equation*}
  \chart = 
  \begin{pmatrix}
    U & R \\
    \zeromatrix & L
  \end{pmatrix}
\end{equation*}
where $U$ is upper triangular and is the chart corresponding to the first part of the string (the \emph{upper} part of the chart), $L$ is upper triangular and is the chart corresponding to the second part of the string (the \emph{lower} part of the chart), and $R$ corresponds to the parses that start in the first string and end in the second string (the \emph{rectangular} part of the chart).

If we put this into the specification, we get:
\begin{equation*}
  \begin{pmatrix}
    \tc{U} & \rc{R} \\
    \zeromatrix   & \tc{L}
  \end{pmatrix}
  =  
  \begin{pmatrix}
    \tc{U} & \rc{R} \\
    \zeromatrix   & \tc{L}
  \end{pmatrix}
  \begin{pmatrix}
    \tc{U} & \rc{R} \\
    \zeromatrix   & \tc{L}
  \end{pmatrix}
  +
  \begin{pmatrix}
    U & R \\
    \zeromatrix & L
  \end{pmatrix}
\end{equation*}
where $\tc{U}$, $\rc{R}$, $\tc{L}$ are the part of $\tc{\chart}$s corresponding to $U$, $R$ and $L$ (a priori, we do not know if $\tc{U}$ and $\tc{L}$ are the transitive closures of $U$, $L$). Performing the multiplication and addition, we get:
\begin{equation*}
    \begin{pmatrix}
    \tc{U} & \rc{R} \\
    \zeromatrix   & \tc{L}
  \end{pmatrix} 
    =
  \begin{pmatrix}
    \tc{U}\tc{U} + \rc{R}\zeromat + U   &   \tc{U}\rc{R}     + \rc{R}\tc{L} + R \\
    \zeromatrix                          &   \zeromat R + \tc{L}\tc{L} + L
  \end{pmatrix}.
\end{equation*}
Since $\zeromat$ is a zero element and since all elements of two matrices need to be equal for the matrices to be equal, we get the set of equations:
\begin{align}
  \tc{U} &= \tc{U}\tc{U} + U \\
  \rc{R} &= \tc{U}\rc{R} + \rc{R}\tc{L} + R \label{Equation:R-Specification}\\
  \tc{L} &= \tc{L}\tc{L} + L,
\end{align}
We see that it the condition that $\tc{\chart}$ is the transitive closure of $\chart$ is equivalent to the conditions that the upper and lower parts of $\tc{\chart}$ are the transitive closures of the upper and lower parts of $\chart$, respectively and the rectangular part of $\tc{\chart}$ satisfies the equation \eqref{Equation:R-Specification}, which we will refer to as the overlap specification. Intuitively, this makes sense, since the transitive closure of the first part describes the ways to get between nodes in the first part, and these do not depend on the second part, and vice versa, since the matrix is upper triangular. To parse a subset of the of the first part of a string, it does not matter what the second part of the string is, because the grammar is context free.
%Hence, if we compute the transitive closures of the upper and lower part of the matrix recursively, we only need to put them together and compute the rectangular part of the matrix.
\subsection{The overlap part}
\label{Section:Two-Step-Splitting}
To compute the overlap part, we need to consider four separate cases, depending on the dimensions of $R$. We will derive a recursive algorithm for computing $\rc{R}$ from $R$, $\tc{U}$ and $\tc{L}$. %function for computing $\rc{R}$ from $R$, $\tc{U}$ and $\tc{L}$, that we will call $\overlap(\tc{U}, R, \tc{L})$ , because when used for parsing, 

First, if $R$ is a $1 \times 1$ matrix, in which case we must have that $\tc{U}$ and $\tc{L}$ are also $1 \times 1$ matrices, and since they are upper triangular, they both equal the zero matrix. Hence, by \eqref{Equation:R-Specification}, $\rc{R} = R$.

We leave out the cases where $R$ is $1 \times n$ or $n \times 1$. They are similar to the case when $R$ is $m \times n$ with both $m$ and $n$ greater than $1$ (and could be derived from it by allowing blocks with $0$ width or height, which we discuss in Section \ref{Section:Empty-Matrices}).

Now, if $R$ is an $m \times n$ matrix, with $m > 1$, $n > 1$, we can subdivide $R$ along both rows and columns, into four blocks ($A$ is an $i \times k$ matrix, $B$ an $i \times l$ matrix and so on):
\begin{equation*}
  R =
  %\begin{pmatrix}
  %  A & B \\ 
  %  C & D
  %\end{pmatrix},
  \begin{blockarray}{ccc}
    & k & l \\
    \begin{block}{c(cc)}
      i & A & B  \\
      j & C & D  \\
    \end{block}
  \end{blockarray}
\end{equation*}
%where $A$ and $B$ both have $i$ rows, $A$ and $C$ have $k$ columns, $C$ and $D$ have $has $i$  \times l$ matrix, $C$ is a $j \times k$ matrix and $D$ is a $j \times l$ matrix, where $i + j = m$, $k + l = n$.
We subdivide $\tc{U}$ and $\tc{L}$ along the same rows and columns:
\begin{equation*}
  \tc{U} = 
\begin{blockarray}{ccc}
    & i & j \\
    \begin{block}{c(cc)}
      i & \tc{U_U} & \rc{U_R} \\
      j & \zm      & \tc{U_L} \\
    \end{block}
  \end{blockarray}
  %\bordermatrix{&i&j\cr
  %i&  \tc{U_U} & \rc{U_R} \cr&i
  %j& \zeromatrix   & \tc{U_L}
  % }
  %\begin{pmatrix}
  %  \tc{U_U} & \rc{U_R} \\
  %  \zeromatrix   & \tc{U_L}
  %\end{pmatrix} 
  \quad \text {and} \quad
  \tc{L} = 
\begin{blockarray}{ccc}
    & k & l \\
    \begin{block}{c(cc)}
      k & \tc{L_U} & \rc{L_R}  \\
      l & \zm & \tc{L_L} \\
    \end{block}
  \end{blockarray}
%\bordermatrix{&k&l \cr
 %   k&\tc{L_U} & \rc{L_R} \cr
  %  l&\zeromatrix   & \tc{L_L}
  % }
  %\begin{pmatrix}
  %  \tc{L_U} & \rc{L_R} \\
  %  \zeromatrix   & \tc{L_L}
  %\end{pmatrix}.
\end{equation*}
%where $\tc{U_U}$ is a $i \times i$ matrix, $\rc{U_R}$ is a $i \times j$ matrix, $\tc{U_L}$ is a $j \times j$ matrix, $\tc{L_U}$ is a $k \times k$ matrix, $\rc{L_R}$ is a $k \times l$ matrix, and $\tc{L_L}$ is a $l \times l$ matrix. 

Inserting this in the overlap specification, \eqref{Equation:R-Specification}, and performing the multiplications gives us,
\begin{align*}
  \begin{pmatrix}
    \rc{A} & \rc{B} \\
    \rc{C} & \rc{D}
  \end{pmatrix} &= 
  \begin{pmatrix}
    \tc{U_U}\rc{A} + \rc{U_R}\rc{C}  &  \tc{U_U}\rc{B} + \rc{U_R}\rc{D} \\
    \tc{U_L}\rc{C}                   &  \tc{U_L}\rc{D}
  \end{pmatrix}
  +\\&+
  \begin{pmatrix}
    \rc{A}\tc{L_U}        &   \rc{A}\rc{L_R} + \rc{B}\tc{L_L}\\
    \rc{C}\tc{L_U}        &   \rc{C}\rc{L_R} + \rc{D}\tc{L_L}
  \end{pmatrix}
  +
  \begin{pmatrix}
    A & B \\
    C & D
  \end{pmatrix}
\end{align*}
where we have again written $\rc{A}$, $\rc{B}$, $\rc{C}$ and $\rc{D}$, for the parts of $\rc{R}$ corresponding to $A$, $B$, $C$ and $D$ (which a priori do not satisfy the overlap specification for anything).
Hence, after rearranging (and inserting parentheses to make things clearer), we get the equations
\begin{align*}
  \rc{A} &= \tc{U_U}\rc{A} + \rc{A}\tc{L_U} + (\rc{U_R}\rc{C} + A) \\
  \rc{B} &= \tc{U_U}\rc{B} + \rc{B}\tc{L_L} + (\rc{U_R}\rc{D} + \rc{A}\rc{L_R} + B) \\
  \rc{C} &= \tc{U_L}\rc{C} + \rc{C}\tc{L_U} + C\\
  \rc{D} &= \tc{U_L}\rc{D} + \rc{D}\tc{L_L} + (\rc{C}\rc{L_R} +  D).
\end{align*}
Now, recall that the overlap specification of $\rc{R}$ \eqref{Equation:R-Specification} contains three parts, something to multiply with on the left, denoted by $\tc{U}$, something to multiply with on the right, denoted by $\tc{L}$ and something to add, denoted by $R$. Looking at the above equations, we see that they state that $\rc{A}$, $\rc{B}$, $\rc{C}$ and $\rc{D}$ satisfy the  overlap specification for some particular choices of $R$, $\tc{U}$ and $\tc{L}$.

The third equation states that  $\rc{C}$ satisfies the overlap specification with $\tc{U} = \tc{U_L}$, $\tc{L} = \tc{L_U}$ and $R = C$. The first equation
states that $\rc{A}$ satisfies it with $\tc{U} = \tc{U_U}$, $\tc{L} = \tc{L_U}$, $R = \rc{U_R}\rc{C} + A$. Similarly for the second and fourth equations.

We see also that the equation for $\rc{B}$ contains $\rc{A}$ and $\rc{D}$. The equations for $\rc{A}$ and $\rc{D}$ in turn both contain $\rc{C}$, while $\rc{C}$ does not contain any of the other parts. So we can compute them recursively, starting with $\rc{C}$, and then computing $\rc{A}$ and $\rc{D}$, finishing with $\rc{B}$. We illustrate the recursion paths in Figure \ref{Fig:Overlap4}.
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \begin{tikzpicture}
      \matrix[matrix of math nodes,left delimiter = (,right delimiter = ),row sep=5pt,column sep = 5pt,nodes={inner sep=0pt,anchor = center, minimum width=7mm, minimum height = 7mm}] (m) {
        \tc{U_U} & \node (m-1-2) [draw=black,shape=circle,radius=3.5mm] {\rc{U_R}}; & A                                                                & B   \\
                 & \tc{U_L}                                                         & \node (m-2-3) [draw=black,shape=circle,radius=3.5mm] {\rc{C}};   & D   \\
                 &                                                                  & \tc{L_U}                                                         & \rc{L_R} \\
                 &                                                                  &                                                                  & \tc{L_L} \\
      };
      \draw (m-1-1.north west) -- (m-1-1.south east) -- (m-1-1.north east) -- cycle;
      \draw (m-3-3.north west) -- (m-3-3.south east) -- (m-3-3.north east) -- cycle;
      \draw (m-1-3.north west) -- (m-1-3.south west) -- (m-1-3.south east) -- (m-1-3.north east) -- cycle;
      \draw[->, thick] (m-1-2) -- (m-2-3);
      
      %\draw (m-2-3.center) circle (3.5mm);
    \end{tikzpicture}
    \subcaption{Computation of $\rc{A}$.}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
\begin{tikzpicture}
\matrix[matrix of math nodes,left delimiter = (,right delimiter = ),row sep=5pt,column sep = 5pt,nodes={inner sep=0pt,anchor = center, minimum width=7mm, minimum height = 7mm}] (n) {
\tc{U_U} & \node (n-1-2) [draw=black,shape=circle,radius=3.5mm] {\rc{U_R}}; & \node (n-1-3) [draw=black,shape=circle,radius=3.5mm] {\rc{A}};   & B   \\
    & \tc{U_L} & \rc{C}   & \node (n-2-4) [draw=black,shape=circle,radius=3.5mm] {\rc{D}};   \\
    &     & \tc{L_U} & \node (n-3-4) [draw=black,shape=circle,radius=3.5mm] {\rc{L_R}}; \\
    &     &     & \tc{L_L} \\
};
\draw (n-1-1.north west) -- (n-1-1.south east) -- (n-1-1.north east) -- cycle;
\draw (n-4-4.north west) -- (n-4-4.south east) -- (n-4-4.north east) -- cycle;
\draw (n-1-4.north west) -- (n-1-4.south west) -- (n-1-4.south east) -- (n-1-4.north east) -- cycle;
\draw [->, thick] (n-1-2) -- (n-2-4);
\draw [->, thick] (n-1-3) -- (n-3-4);
%\draw (n-1-3.center) circle (3.5*1.4142mm);
%\draw (n-3-4.center) circle (3.5*1.4142mm);
%\draw (n-1-2.center) circle (3.5*1.4142mm);
%\draw (n-2-4.center) circle (3.5*1.4142mm);
\end{tikzpicture}
    \subcaption{Computation of $\rc{B}$.}
  \end{subfigure}

  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \begin{tikzpicture}
      \matrix[matrix of math nodes,left delimiter = (,right delimiter = ),row sep=5pt,column sep = 5pt,nodes={inner sep=0pt,anchor = center, minimum width=7mm, minimum height = 7mm}] (a) {
        \tc{U_U} & \rc{U_R} & A   & B   \\
           & \tc{U_L} & C   & D   \\
           &    & \tc{L_U} & \rc{L_R} \\
           &    &    & \tc{L_L} \\
      };
      \draw (a-2-2.north west) -- (a-2-2.south east) -- (a-2-2.north east) -- cycle;
      \draw (a-3-3.north west) -- (a-3-3.south east) -- (a-3-3.north east) -- cycle;
      \draw (a-2-3.north west) -- (a-2-3.south west) -- (a-2-3.south east) -- (a-2-3.north east) -- cycle;
    \end{tikzpicture}
    \subcaption{Computation of $\rc{C}$.}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
\begin{tikzpicture}
\matrix[matrix of math nodes,left delimiter = (,right delimiter = ),row sep=5pt,column sep = 5pt,nodes={inner sep=0pt,anchor = center, minimum width=7mm, minimum height = 7mm}] (m) {
\tc{U_U} & \rc{U_R} & A   & B   \\
    & \tc{U_L} & \node (m-2-3) [draw=black,shape=circle,radius=3.5mm] {\rc{C}};   & D   \\
    &     & \tc{L_U} & \node (m-3-4) [draw=black,shape=circle,radius=3.5mm] {\rc{L_R}}; \\
    &     &     & \tc{L_L} \\
};
\draw (m-2-2.north west) -- (m-2-2.south east) -- (m-2-2.north east) -- cycle;
\draw (m-4-4.north west) -- (m-4-4.south east) -- (m-4-4.north east) -- cycle;
\draw (m-2-4.north west) -- (m-2-4.south west) -- (m-2-4.south east) -- (m-2-4.north east) -- cycle;
\draw [->, thick] (m-2-3) -- (m-3-4);
%draw (m-3-4.center) circle (3.5*1.4142mm);
%draw (m-2-3.center) circle (3.5*1.4142mm);
\end{tikzpicture}
    \subcaption{Computation of $\rc{D}$.}
  \end{subfigure}
\caption{Illustration of the overlap step.\label{Fig:Overlap4} Overlap is called on the two triangles and the square to which we add any circled parts, multiplied according to the arrows.}
\end{figure}
\subsection{Summary of Valiant's algorithm}
\label{Valiant-summing-up}
Summing up, we give the algorithm for computing the transitive closure $\tc{\chart}$ of an upper triangular matrix $\chart$ here (including the cases where the rectangular part is a row or column vector) as a function $\valiant(\chart)$:
\begin{itemize}
\item If $\chart$ has size $1 \times 1$, return $\chart$.
\item Otherwise $\chart$ has size $n \times n$, with $n > 1$. Split $\chart$ into
  \begin{equation*}
    \chart = 
    \begin{pmatrix}
      U           &  R \\
      \zeromatrix &  L
    \end{pmatrix},
  \end{equation*}
  and then: 
  \begin{enumerate}
  \item Recursively compute $\tc{U} = \valiant(U)$ and $\tc{L} = \valiant(L)$.
  \item Compute the overlap $\rc{R}$ with a function $\overlap(\tc{U}, R, \tc{L})$ defined by:
    \begin{itemize}
    \item If $R$ has size $1 \times 1$, return $R$.
    \item If $R$ has size $n \times 1$, $n > 1$, then $\tc{L}$ is the $1\times 1$ zero matrix. Split $\tc{U}$ and $R$ into
      \begin{equation*}
        \tc{U} = 
        \begin{pmatrix}
          \tc{U_U} & \rc{U_R} \\
          \zm      & \tc{U_L} 
        \end{pmatrix} 
        \quad \text{and}\quad
        R = 
        \begin{pmatrix}
          u \\
          v
        \end{pmatrix}
      \end{equation*}
        and recursively compute 
        \begin{align*}
          \rc{v} &= \overlap(\tc{U_L}, v, \zeromat)\\
          \rc{u} &= \overlap(\tc{U_U}, \rc{U_R}\rc{v} + u, \zeromat).
        \end{align*}
        Return $\begin{pmatrix}
            \rc{u}\\
            \rc{v}
          \end{pmatrix}$.
        \item If $R$ has size $1 \times n$, $n > 1$, then $\tc{U}$ is the $1 \times 1$ zero matrix. Split $R$ and $\tc{L}$ into 
          \begin{equation*}
            R = 
            \begin{pmatrix}
              u & v
            \end{pmatrix}
            \quad \text{and}\quad
            \tc{L} = 
            \begin{pmatrix}
              \tc{L_U} & \rc{L_R}\\
              \zm   & \tc{L_L} 
            \end{pmatrix}
          \end{equation*}
          and recursively compute 
          \begin{align*}
            \rc{u} &= \overlap(\zm,u,\tc{L_U})\\
            \rc{v} &= \overlap(\zm,\rc{u}\rc{L_R} + v,\tc{L_L}).
          \end{align*} 
          Return $
            \begin{pmatrix}
              \rc{u} & \rc{v}
            \end{pmatrix}$.
        \item Otherwise, $R$ has size $m \times n$ with $m > 1$ and $n > 1$. Split $\tc{U}$, $R$ and $\tc{L}$ into 
          \begin{equation*}
            \tc{U} = 
            \begin{pmatrix}
              \tc{U_U} & \rc{U_R} \\
              \zm      & \tc{U_L} 
            \end{pmatrix}
            \quad \text{and}\quad
            R = 
            \begin{pmatrix}
              A & B \\
              C & D
            \end{pmatrix}
            \quad \text{and}\quad
            \tc{L} = 
            \begin{pmatrix}
              \tc{L_U} & \rc{L_R}\\
              \zm   & \tc{L_L} 
            \end{pmatrix}
          \end{equation*}
          and recursively compute 
          \begin{align*}
            \rc{C} &= \overlap(\tc{U_L},C,\tc{L_U}) \\
            \rc{A} &= \overlap(\tc{U_U},\rc{U_R}\rc{C} + A,\tc{L_U}) \\
            \rc{D} &= \overlap(\tc{U_L},\rc{C}\rc{L_R} + D,\tc{L_L}) \\
            \rc{B} &= \overlap(\tc{U_U},\rc{U_R}\rc{D} + \rc{A}\rc{L_R} + B,\tc{L_L}).
          \end{align*}
         Return $
            \begin{pmatrix}
              \rc{A} & \rc{B} \\
              \rc{C} & \rc{D}
            \end{pmatrix}$.
    \end{itemize}
    \item Return $
        \begin{pmatrix}
          \tc{U} & \rc{R} \\
          \zm    & \tc{L}
        \end{pmatrix}$.
  \end{enumerate}
\end{itemize}
\section{Datatypes}
In this section, we are going to define datatypes to use when we implement Valiant's algorithm in \ref{Implementation}.
\subsection{Discussion}
To implement this in Agda using the |Matrix| and |Triangle| datatypes from Sections \ref{Section:Matrices} and \ref{Section:Triangle} would be very complicated since we would have to handle the splitting and triangularity proofs manually. Instead, we define concrete representations for the matrices and triangle that have the way we split them built in.
We will call the datatypes we use |Mat| and |Tri| for general matrices and upper triangular matrices, respectively.
To build the split into the data types, we give them constructors for building a large |Mat| or |Tri| from four smaller |Mat|s or two |Tri| and one |Mat| respectively. We begin with |Mat| since it is needed to define |Tri|. 

By the above reasoning, we have one constructor, which we will call |quad| that takes four smaller matrices and puts them together into a big one. Written mathematically, we want the meaning to be:
\begin{equation*}
\operatorname{quad}(A,B,C,D) = 
\begin{pmatrix} 
  A & B \\
  C & D
\end{pmatrix},
\end{equation*}
where $A$ has the same number of rows as $B$, $C$ has the same number of rows as $D$, $A$ has the same number of columns as $C$ and $B$ has the same number of columns as $D$.
Thinking about what ``small'' structures should have constructors, we should definitely have a constructor |sing| for single element matrices. We realize that it is not enough to simply allow these $1 \times 1$ matrices, as it would imply that any matrix is a $2^n \times 2^n$ matrix, where $n$ is the number of times we use |quad|. 

\label{Section:Empty-Matrices}
One way to allow arbitrary dimensions for the matrices is to have a constructor |empty| for ``empty'' matrices of any dimension, that play two different roles. First, empty $0 \times n$ matrices are used to allow |quad| to put two matrices with the same number of rows next to each others:
\begin{equation}
\operatorname{quad}(A, B, e_{0\,m}, e_{0\,n}) =
\begin{pmatrix}
  A & B \\
  e_{0\,m} & e_{0\,n}
\end{pmatrix} = (A,  B),
\end{equation}
where $e_m$ and $e_n$ are empty $m \times 0$ and $n\times 0$ matrices respectively. Similarly, empty $n \times 0$ matrices are used to put two matrices with the same number of columns on top of each others. Second, an empty $m \times n$ matrix, $m > 0$, $n > 0$, represents a $m \times n$ matrix whose entries are all zero. One advantages of this method is that we could give fast implementations of  addition and multiplication of ``empty'' matrices:
\begin{align*}
  e_{m\, n} + A &= A \\ 
  A + e_{m\,n}  &= A \\
  e_{m\, n}   A &= e_{m\,p} \\
  A e_{n\,p}    &= e_{m\,p},
\end{align*}
where $A$ is an arbitrary $m \times n$, $n \times p$ and $m \times n$ matrix, respectively.
Another is that it keeps the number of constructors down (three constructors for the matrix type), and this is desirable when proving things with Agda, since we generally pattern match on the structures, and this gives us a case for each constructor.

One downside is that there are multiple constructors for matrices of the same size (there is always |empty| and a nonempty way when $m > 0$, $n > 0$), removing some of the advantage we get from having few constructors.

Another approach, which is the one we take in this report, is to have constructors for row and column vectors, $1 \times n$ and $n \times 1$ matrices for arbitrary $n > 1$, along with the single element matrices. We define |rVec| and |cVec| to take a vector of length $n > 1$ and turn it into a $1 \times n$ or $n \times 1$ matrix respectively.
This approach has the advantage that we can define all matrices in a simple way, and that we could potentially specialise algorithms when the input is a vector, but introduces one extra constructor (|sing|, |rVec|, |cVec| and |quad| compared to |empty|, |sing| and |quad|).

We then need a concrete representation |Vec| of vectors. Since we want to be able to split vectors along the middle (to implement Valiant's algorithm in the case where $R$ is a vector), we give them a constructor |two| that takes a vector of length $m$ and one of length $n$ and concatenates them. For our base cases, we need to be able to build single element vectors, this is enough to build any vector.
To implement this approach, we need to define the datatypes |Vec| of vectors and |Mat| of matrices (that should be concrete representations of |Vector| and |Matrix|).
\subsection{A first attempt at an Agda implementation}
%include Code/Valiant/NaiveCode.lagda
\subsection{|Mat| and |Tri|}
Instead we want to use an approach for indexing our matrices where we build the splitting further into the datatypes. Looking at the first attempt to define |quad|, we see that we only use constant |ℕ|s and |_+_| (we never use |suc| to increase the size of a matrix). So we could use a datatype like |ℕ|, but, instead of having |suc| as a constructor, it has |_+_|.
%include Code/Valiant/Splitting.lagda

Using this datatype we can finally define our datatypes |Mat| and |Tri|.
%include Code/Valiant/MatAndTri.lagda
%include Code/Valiant/Operations.lagda
%include Code/Valiant/NANRings.lagda
\section{Implementation and proof of correctness}
We are now ready to implement the algorithm in Agda.
\label{Implementation}
%include Code/Valiant/Specification.lagda
