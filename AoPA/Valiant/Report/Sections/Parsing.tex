\newcommand{\productions}{P}
\newcommand{\nonterminals}{N}
\newcommand{\terminals}{\Sigma}
\newcommand{\startsymbol}{S}
\newcommand{\grammar}{(\nonterminals, \terminals, \productions, \startsymbol)}
\section{Parsing}
Parsing is about analysing the structure of a sequence of tokens coming from some alphabet. We only give a brief overview of the problem here.
We begin by introducing some concepts of parsing in Section \ref{Parsing-Defs}. Then, in Section \ref{Parsing-Algebra}, we tie these concept together wwith the algebra from Section \ref{Section:Algebra}, and finally, in Section \ref{Parsing-TC}, we show that parsing is equivalent to computing the transitive closure of an upper triangular matrix.
In Section \ref{Section:Valiant}, we will then focus on a particular algorithm for computing the transitive closure, Valiant's algorithm, that we implement and prove the correctness of using Agda.

\subsection{Definitions}
The goal of parsing is first to decide if a given sequence of tokens belongs to a given language, and second to describe its structure in the language.
For this, we consider the opposite process of generating a string in a given language. To do this, one uses a grammar for the language, which contains rules that can be used to build strings belonging to the language. %, or to try assign structural properties to a sequences of tokens.
\begin{Definition}
  A \emph{grammar} $G$ is a tuple $\grammar$, where 
  \begin{itemize}
  \item $\nonterminals$ is a finite set of nonterminals.
  \item $\terminals$, is a finite set of terminals, with $\nonterminals \cap \terminals = \emptyset$.
  \item $\productions$, is a finite set of production rules, written as $\alpha \to \beta$, where $\alpha$ and $\beta$ are sequences of terminals and nonterminals, and $\alpha$ contains at least one nonterminal.
  \item $\startsymbol \in \nonterminals$ is the start symbol.
  \end{itemize}
  We use upper case letters to denote nonterminals, lower case letters to denote terminals and Greek letters to denote sequences of both terminals and nonterminals.
\end{Definition}
A grammar generates a string of terminals by repeatedly applying production rules to the start symbol.
The language generated by a grammar is the set of strings of tokens it generates.

Parsing is then the process of taking a string and figuring out what (if any) sequence of expansions might have produced it. Often, one creates a datastructure annotating the string with the nonterminals generating the parts of the string.
\begin{Example}
  \label{Arithmetic}
  We present a simple grammar for a language of arithmetic expressions (which appears in slightly modified form in \cite{Lange-Leiss}) and give an example of string generation, and one of parsing: 
  \begin{itemize}
  \item $\terminals = \{1,\ldots , 9, +, *, (, )\}$,
  \item $\nonterminals = \{E, T, F, N\}$, for ``expression'', ``term'', ``factor'' and ``number'', respectively.
  \item The production rules are
    \begin{itemize}
    \item $E \to T$
    \item $E \to E + T$
    \item $T \to F$
    \item $T \to T * F$ 
    \item $F \to ( E )$
    \item $F \to N$
    \item $N \to i$, for $i = 1$, \ldots $9$. 
    \end{itemize}
  \item $S = E$.
  \end{itemize}
  To generate a string, we begin with $E$, 
\end{Example}
We are not going to consider arbitrary grammars in this report, so we give two restrictions to the definition above: 
\begin{Definition}
  A grammar is \emph{context free} if the left hand side of every production rule is a single nonterminal: $A \to \beta$.
\end{Definition}
\begin{Definition}
  A grammar is in (reduced) Chomsky Normal Form \cite{Chomsky} if the every production rule is of one of the following two forms:
  \begin{align*}
  A &\to BC \\
  A &\to a
  \end{align*}
\end{Definition}
It is well known that any Context Free Grammar can be converted into a grammar in CNF (which generates the same language), with a size increase that is at most quadratic (the size of a grammar is the number of symbols contained \cite{Lange-Leiss}.
In the remainder of the report, we only consider grammars in Chomsky Normal Form.
\begin{Example}
  \label{CNF-Ex}
  We give a grammar in Chomsky Normal Form generating the same language as the one in Example \ref{Arithmetic}:
\end{Example}
\subsection{Grammar as a nonassociative semiring}
The set of production rules for a grammar in Chomsky Normal Form which have the form $A \to BC$ could almost be used as a definition of multiplication among the nonterminals by replacing the arrows by equals signs:
\begin{equation*}
  BC = A
\end{equation*}
However, there are two problems with this definition. First, as we see in Example \ref{CNF-Ex}, there can be nonterminals $B$ and $C$ with no production $A \to BC$ (...). Second, there can be many different nonterminals that expand to the same thing: there can be $A_1$ and $A_2$ such that $A_1 \to BC$ and $A_2 \to BC$ are both productions.

The solution to these two problems is to instead consider \emph{sets} of nonterminals, with the following multiplication:
\begin{equation*}
  x \cdot y = \{A \st \exists B \in x,\, C \in y,\, A \to BC \in P\}.
\end{equation*}

In general, this multiplication does not satisfy any algebraic axioms, it is not... \todo{THOMAS: refer back to example above -- once finished}
\subsubsection{Parsing as Transitive Closure}
In this section, reformulate the parsing problem for a grammar in CNF form as the problem of finding the transitive closure of an upper triangle matrix.

Valiant showed this when defining his algorithm in \cite{Valiant}, but he uses a different specification of the transitive closure, which is  less suitable for use in Agda.
% here should be a ``proof'' that parsing can be seen as computing the transitive closure of a matrix
When we consider the 

\subsection{Specification of non-associative transitive closure}
In Section \ref{Parsing as transitive closure}, we showed that parsing can be considered as computing the (non-associative) transitive closure of a matrix. To get an algorithm from this, we want to create a specification for the problem of finding the (non-associative) transitive closure of a matrix. As before, we let $C^+$ denote the transitive closure of $C$. In \cite{Valiant}, the specification is given as
\begin{equation}
  \label{VSpec}
  C^+ = \sum_{i = 1}^{\infty}\nap{C}{i}
\end{equation}
where $\nap{C}{n}$ is defined recursively by:
\begin{equation}
  \nap{C}{1} = C,
\end{equation}
\begin{equation}
  \label{VSpec-rec}
  \nap{C}{n} = \sum_{i = 1}^n(\nap{C}{i})(\nap{C}{n-i}).
\end{equation}
Hence, $\nap{C}{n}$ is the sum of all possible bracketings of products containing $n$ copies of $C$ (this is the $n$th Catalan number, $(2n)!/(n!(n+1)!)$ \cite{mathworld-catalan}).
The idea behind the specification, and the justification that it specifies the transitive closure of $C$, is that $c_{ij} \ge x$ if there is ? an element belongs in the matrix if it \todo{THOMAS: Continue --- mention reduction when associative, looks good, because it looks lika a calculation}.

We note that it is enough to only consider the sum from $1$ to $n$, since the matrix is upper triangular and hence $\nap{C}{m} = \zeromatrix$ when $m > n$.

However, working with this specification is complicated, and seems to require considering individual matrix elements, and how an element was formed in previous steps (for example, when Valiant proves the correctness of his algorithm, he looks at an arbitrary matrix element and its bracketing \cite{Valiant} \todo{THOMAS: Check valiant --- and expand on bracketing stuff}). 

This specification is not easy to work with in Agda for a number of reasons:
\begin{itemize}
\item The sum \eqref{VSpec} is finite and doesn't make sense.
\item The other source of recursion: \eqref{VSpec-req} is complicated.
\item The proof by Valiant would probably be hard to adapt since it includes even more consepts, moving away from our algebraic structure view---considers bracketings of elements.
\item Other reasons?
\end{itemize}

A big problem with the above specification along with Valiant's proof using it, is that it was too concerned by syntactical matters (how a particular element of the transitive closure was built with regards to where the parentheses were placed), which we want to abstract away by considering algebraic structures (which, once a product is formed, destroy the information as to how it was formed). \todo{JPPJ: Deep/Shallow embedding}. So we move towards a more semantical specification of the problem of computing the transitive closure.

The simple fact we use is that if an element belongs to the transitive closure $C^+$, then it either belonged to $C$, or it must also belong to $C^+C^+$.\todo{Expand / prove} This gives us the following specification:
\begin{equation}
  \label{JPTSPec}
  C^+ = C^+C^+ + C
\end{equation}
\todo{THOMAS: words to use to refer to the elements of the matrices}
We note that this specification doesn't explicitly mention the non-associativity of multiplication. If we expand the equation once (by replacing $C^+$ on the right hand side by $C^+C^+ + C$), we get
\begin{equation}
  C^+ = (C^+C^+ + C)(C^+C^+ + C) + C = (C^+C^+)(C^+C^+) + (C^+C^+)C + C(C^+C^+) + CC + C,
\end{equation}
and if we repeat this again, we get
\begin{equation}
  C^+ = ((C^+C^+ + C)(C^+C^+ + C))((C^+C^+ + C)(C^+C^+ + C)) + ((C^+C^+ + C)(C^+C^+ + C))C + C((C^+C^+ + C)(C^+C^+ + C)) + CC + C = .
\end{equation}\todo{THOMAS: Expand, maybe, or remove equation}
We prove that the two specifications are equivalent \eqref{VSpec} and \eqref{JPTSpec}:
\begin{Theorem}
The transitive closure $C^+$ of $C$ satisfies \todo{THOMAS: Need upper triang here?} 
\begin{equation}
  \label{JPTSpec'}
  C^+ = C^+C^+ + C
\end{equation}
if and only if it satisfies
\begin{equation}
  \label{VSpec'}
  C^+ = \sum_{i = 1}^{\infty}\nap{C}{i}
\end{equation}
\end{Theorem}
\begin{proof}
We prove this by showing by induction on the index $i$ in the sum \eqref{VSpec'} that if $C^+$ satisfies \eqref{JPTSpec'}, then the products of $i$ copies of $C$ contain all possible bracketings of the factors. For $i = 1$, this is obvious since the terms resulting from $C^+C^+$ contain at least two $C$s. If it is true for $i < k$, then for $i = k$, \todo{THOMAS: finish} 
\end{proof}

This specification turns out to work very well with Agda (once we define appropriate concrete datatypes for the matrices---which will be different from the abstract ones given in Section \ref{matrix-datatype}---we will do this in Section \ref{datatype-section}).

To end this section, we note that other possible specifications of the transitive closure, similar to \eqref{JPTSpec}, that are equivalent to it if we assume associativity (and that addition is idempotent) fail to be correct without associativity, one such is:
\begin{equation}
  C^+ = C^+C + C,
\end{equation}
and adding the extra term $CC^+$, to get
\begin{equation}
  C^+ = C^+C + CC^+ + C
\end{equation}
fails to make the specification correct, since when expanding them, these two only ever produce bracketings of the form $(\cdots(CC)\cdots) C$ (and $C(\cdots(CC)\cdots)$.

\todo{fix references to other sections}
\label{Section:Magma-multiplication}
