\section{Algebra}
\label{Algebra}
\label{Section:Algebra}
In this section, we are going to introduce a number of algebraic definitions. Some (like commutative monoid and nonassociative semiring) will be useful later in the report, while other (like group and ring) are mentioned as possibly familiar examples and for comparison.

In Section \ref{Intro-defs} we introduce a number of well known propositions that show up in the definitions of algebraic structures as axioms and comment on differences between defining an algebraic structure in mathematics and in Agda. Then, in Sections \ref{Monoid} and \ref{Ring}, we use these properties to define algebraic structures consisting of sets with one and two binary operations, respectively. It might be useful to look at for example Definition \ref{Group} of a group before proceeding.
\subsection{Introductory definitions}
\ref{Intro-defs}
When defining an algebraic structure (consisting of just one set), one gives the set of objects, a number of binary operations on the objects and a number of axioms that the set and the operations are required to satisfy. In this section, we are going to introduce common such axioms. 
\subsubsection{Equivalence relations}
\label{Algebra-equality}
The axioms usually refer to equalities between different sequences of operation applications, like the axiom in a group that $x \cdot (y \cdot z) = (x \cdot y) \cdot z$ for all $x$, $y$ and $z$. However, to define these things in Agda, we note that we do not have a concept of equality for terms of an arbitrary datatype. Further, the most ``basic'' equality definition (called propositional equality), stating just that $x = x$ for all $x$ and defined by
\begin{code}
data _≡_ {A : Set} : A → A → Set where
  refl : {x : A} → x ≡ x
\end{code}
is often not what we want. For the datatype |ℕ|, it coincides with the ``mathematical'' equality of natural numbers, because |ℕ| is inductively defined (in particular, if two things are built in different ways by |zero| and |suc|, they cannot be equal). But if we were to define the integers |ℤ| as differences between natural numbers:
\begin{code}
data ℤ : Set where
  _-_ : ℕ → ℕ → ℤ
\end{code}
we want a concept of equality that conciders |5 - 3| as equal to |0 - 2|, for example. Also, if we define a datatype of sets, we want sets where the elements have been added in different orders to be equal. Hence, we need to generalize the propositional equality to some kind of relation that behaves well. We cannot choose an arbitrary relation, for example, $<$ does not behave as we expect equality to. 

It turns out that there are three properties we want a concept of equality to have. First, it should be reflexive: every element should be equal to itself. Second, it should be symmetric: if $a$ is equal to $b$, $b$ should be equal to $a$. Third, it should be transitive: if $a$ is equal to $b$ and $b$ is equal to $c$, then $a$ should be equal to $c$. A relation that satisfies this is called an equivalence relation:
\begin{Definition}
  A relation $\sim \subset X \times X$ is called an \emph{equivalence relation} if it is
  \begin{itemize}
  \item Reflexive: for $x \in X$, $x \sim x$.
  \item Symmetric: for $x$, $y \in X$, if $x \sim y$, then $y \sim x$.
  \item Transitive: for $x$, $y$, $z \in X$, if $x R y$ and $y R z$, then $x R z$.
  \end{itemize}
\end{Definition}
The following proposition formalizes the way it behaves like an equality (for a proof, see for example \cite{Equivalence-proof}):
\begin{Proposition}
An equivalence relation $R$ partitions the elements of a set $X$ into disjoint nonempty equivalence classes (subsets $[x] = \{y \in X \st y R x\}$) satisfying: 
\begin{itemize}
\item For every $x \in X$, $x \in [x]$.
\item If $x \in [y]$, then $[x] = [y]$.
\end{itemize}
\end{Proposition}
This means that if we use an equivalence relation as ``equality'' on a set, we are saying that two elements are equal if they generate the same equivalence class, so we let actual equality on the equivalence classes give us an ``equality'' on the members of the set.

%include /Code/Algebra/Equivalence.lagda

\todo{Chase down the last few underscores-- there are many in valiant!.}



\subsubsection{Propositions about one operation}
Next, we define three propositions that binary operations (i.e., functions $X \to X \to X$) can satisfy. These are operations that are similar to ordinary addition and multiplication of numbers.
\begin{Definition} %%% Associative
A binary operation $\cdot$ on a set $X$ is \emph{associative} if $x \cdot (y \cdot z) = (x \cdot y) \cdot z$ for all $x$, $y$, $z \in X$.
\end{Definition}
In Agda, the proposition that |_∙_| is associative, with respect to a given equivalence relation |_≈_| is given by the type:
%include Code/Algebra/Short/Associative.lagda

Many familiar basic mathematical operations, like addition and multiplication of numbers, are associative. On the other hand, operations like subtraction and division are not, since $x - (y - z) = x - y + z \ne (x - y) - z$ (but this is because they are in some sense the combination of addition and inversion: $x - y = x + (-y)$). In functional programming, a very important example of an associative operation is list concatenation.
In this thesis, we are very interested in a non-associative operation related to parsing. %remove sentence?



\begin{Definition} %%% Commutative
A binary operation $\cdot$ on a set $X$ is \emph{commutative} if $x \cdot y = y \cdot x$ for all $x$, $y \in X$.
\end{Definition}
In Agda, this proposition is given by the type
%include Code/Algebra/Short/Commutative.lagda
Again, many familiar basic mathematical operations are commutative, like addition and multiplication of numbers, but matrix multiplication (for matrices of size $n \times n$, $n \ge 2$) is an example of an operation that is not commutative.
In this thesis, we are interested in the commutative operation set union, $\cup$.

Now, we present two properties that relate elements to the operations:
\begin{Definition} %%% NEUTRAL ELEMENT!
  An element $e \in X$ is an \emph{identity element} of a binary operation $\cdot$ if $x \cdot e = e \cdot x = x$ for all $x \in X$.
\end{Definition}
In Agda, the type of this proposition is
%include Code/Algebra/Short/Identity.lagda


\begin{Definition}
An element $x^{-1} \in X$ is the \emph{inverse} of $x$ with respect to a binary operation $\cdot$ if $x^{-1} \cdot x = x \cdot x^{-1} = e$.
\end{Definition}
When discussing inverses, it is usually required that every (or nearly every) element has an inverse.
In Agda, since the proposition that every element has an inverse is a universal quantification, it is proved by a function |_⁻¹| that takes an element to its inverse. Given such a function, the statement that |x ⁻¹| is the inverse of |x| is given by the type
%include Code/Algebra/Short/Inverse.lagda

If the operation is addition of numbers, the inverse of $x$ is given by $-x$, and if the operation is multiplication of numbers, the inverse is given by $1/x$. In computer science, inverses occur more rarely.
Indeed, none of the algebraic structures we have used in to prove the correctness of Valiant's algorithm include inverses.

\subsubsection{Propositions about two operations}
\label{two-operations}
When we have two different binary operations on the same set, we often want them to interact with each other sensibly. Here we define two such ways of interaction.

%%%%%%%%%%%%%%%%%%
%% Distributivity
%%%%%%%%%%%%%%%%%%
We recall the distributive law $x\cdot(y + z) = x\cdot y + x\cdot z$, where $x$, $y$, and $z$ are numbers and $\cdot$ and $+$ are multiplication and addition, respectively and generalize it to an arbitrary pair operations:
\begin{Definition}
  A binary operation $\cdot$ on $X$ \emph{distributes over} a binary operation $+$ if, for all $x$, $y$, $z \in X$,
  \begin{itemize}
  \item $x \cdot (y + z) = x \cdot y + x \cdot z$,
  \item $(y + z) \cdot x = y \cdot x + z \cdot x$,
  \end{itemize}
  where we assume that $\cdot$ binds its arguments tighter than $+$.
\end{Definition}
%include /Code/Algebra/Short/Distributive.lagda

%%%%%%%%%%%%%%%%%%
%% Zero
%%%%%%%%%%%%%%%%%%
\label{zero-element}
The second such interaction we will consider comes from the fact that $0$ absorbs when involved in a multiplication of numbers: $0 \cdot x = x \cdot 0 = 0$. The reason we consider this a property of a pair of operations is that if we have an operation $+$ to which we have an identity element $0$ and every element has an inverse, and an operation $\cdot$ which distributes over $+$, we automatically get $0 \cdot x = 0$:
\begin{equation*}
  0 \cdot x = (0 + 0) \cdot x = 0 \cdot x + 0 \cdot x,
\end{equation*}
where the first equality follows from the fact that $0$ is an identity element for $+$, and the second from that $\cdot$ distributes over $+$. We can then cancel $0 \cdot x$ on both sides to get $0 = 0 \cdot x$.

If we do not have inverses, we cannot perform the final step (for example, if $+$ would happen to be idempotent: $x + x = x$ for all $x \in X$, like set union $\cup$ is, we could not conclude that $0 = 0 \cdot x$), and then, it makes sense to have the following as an axiom:
\begin{Definition}
  An element $z \in X$ is a \emph{zero element} (also known as an \emph{absorbing element}) of a binary operation $\cdot$ if $z \cdot x = x \cdot z = z$ for every $x \in X$.
\end{Definition}
%include Code/Algebra/Short/Zero.lagda

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% GROUPS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sets with one operation} % alternate title: genralizations
In this section, we are going to discuss algebraic structures made up of a set and a binary operation on the set.
\subsubsection{Groups}
In mathematics, the most common such structure is the group. Many mathematical objects form groups, including the integers $\Z$, the rational numbers $\Q$, the real numbers $\R$ and the complex numbers $\C$, with addition as the binary opertaion, and the non-zero rational numbers $\Q\setminus{0}$, non-zero real numbers $\R\setminus{0}$, and non-zero complex numbers $\C\setminus{0}$, with multiplication as the binary operation. Although, groups satisfy too many axioms to be useful to us, we give the definition below, to clarify the difference between them and the perhaps less familiar structure, the monoid, that we present next.
\begin{Definition}
A \emph{group} is a set $G$ together with a binary operation $\cdot$ on $G$, satisfying the following:
\begin{itemize}
\item The operation $\cdot$ is associative: for all $x$, $y$, $z \in G$, $(x \cdot y) \cdot z = x \cdot (y \cdot z)$. 
\item There is an identity element $e \in G$: for all $x \in G$, $e \cdot x = x \cdot e = x$.
\item For every $x \in G$, there is an inverse element element $x^{-1}$: $x \cdot x^{-1} = x^{-1} \cdot x = e$.
\end{itemize}
A group where the binary operation is commutative is said to be \emph{Abelian}.
\end{Definition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MONOIDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Monoids}
In this section, we discuss monoids, which are slightly more general than groups in that they do not require the existence of inverses. Monoids are important for programming because it is rare that a datatype satisfies all the axioms of a group.

Example of a monoids (that are not also groups) include the natural numbers, with $\cdot$ as either multiplication or addition, sets ($M$ is a collection of sets, $\cdot$ is union and the identity element is the empty set), lists ($M$ is a collection of lists, $\cdot$ is list concatenation and the identity element is the empty list).

\begin{Definition}
  A monoid is a set $M$, together with a binary operation $\cdot$ on $M$, that satisfies the following:
  \begin{itemize}
  \item The operation $\cdot$ is associative: $x$, $y$, $z \in M$, $(x \cdot y) \cdot z = x \cdot (y \cdot z)$. 
  \item There is an identity element $e \in M$: for all $x \in M$, $e \cdot x = x \cdot e = x$.
  \end{itemize}
A monoid where the binary operation is commutative is a \emph{commutative monoid}.
\end{Definition}
%include Code/Algebra/Monoid.lagda



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% RING LIKE STRUCTURES (two operations on a set)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sets with two operations}
In this section, we discuss algebraic structures that consist of a set together with two binary operations. As we discussed in Section \ref{two-operations}, we want them to interact sensibly.
\subsubsection{Rings}
The basic example of this kind of structure is a ring:
\begin{Definition}
A set $R$ together with two binary operations $+$ and $*$ (called addition and multiplication) forms a \emph{ring} if:
\begin{itemize}
\item It is an Abelian group with respect to $+$.
\item It is a monoid with respect to $*$.
\item $*$ distributes over $+$.
\end{itemize}
\end{Definition}
Examples of rings include the integers, real and complex numbers with the usual addition and multiplication.

However, for the applications we have in mind, Parsing, the algebraic structure in question (see Section \ref{Parsing-as-algebraic structure}) does not even have associative multiplication, and does not have inverses for addition. We still have an additive $0$ (the empty set---representing no parse), which is a zero element with regard to multiplication (if the left, say, substring has no parse, then the whole string has no parse). We also do not have a unit element for multiplication (there is no guarantee that there is a string $A$ such that $A$ followed by $X$ has the same parse as $X$ for every string $X$). The usual proof that $0$ is an absorbing element depends on the existence of the ability to cancel (which is implied by the existence of additive inverses in a group), as seen in section \ref{zero-element}. So if we are to define an algebraic structure modelling this, we need to include as an axiom that zero absorbs.

There are a number of fairly standard \todo{THOMAS: Cite something} generalizations of a ring, but none of these matches our requirements. One generalization is the semiring, which has the same axioms as a ring, except that addition need not have inverses:
\begin{Definition}
A set $R$ together with two binary operations $+$ and $*$ forms a \emph{semiring} if:
\begin{itemize}
\item It is a commutative monoid with respect to $+$.
\item It is a monoid with respect to $*$.
\item $*$ distributes over $+$.
\item $0 * x = x * 0 = 0$ for all $x \in R$.
\end{itemize}
\end{Definition}
Another generalization is the nonassociative ring, which instead does away with the requirement that multiplication is associative and that there is an identity element for multiplication.
\begin{Definition}
A set $R$ together with two binary operations $+$ and $*$ forms a \emph{non-associative ring} if:
\begin{itemize}
\item It is an Abelian group with respect to $+$.
\item It is closed under $*$.
\item $*$ distributes over $+$.
\end{itemize}
\end{Definition}
We take this to mean that the modifier \emph{nonassociative} removes the requirement that the set together with $*$ is a monoid from the axioms of the structure. Hence, we make the following (nonstandard) definition:
\begin{Definition}
A set $R$ together with two binary operations $+$ and $*$ forms a \emph{nonassociative semiring} if:
\begin{itemize}
\item It is a commutative monoid with respect to $+$.
\item $*$ distributes over $+$.
\item $0 * x = x * 0 = 0$ for all $x \in R$.
\end{itemize}
\end{Definition}
%include Code/Algebra/NANRing.lagda

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MATRICES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Matrices}
A matrix is in some sense really just a set of numbers arranged in a rectangle, so there is nothing stopping us from defining such a matrix with entries from an arbitrary set, as opposed to from $\R$ or $\C$. To be similar to the definition we will make in Agda of an abstract matrix (one without a specific implementation in mind), we consider a matrix of size $m \times n$ as a function from a pair of natural numbers $(i,j)$, with $0 \le i < m$, $0 \le j < n$ (or, $i \in \Fin{m}$, $j \in \Fin{n}$, and hence, after currying define: 
\begin{Definition}
A \emph{matrix} $A$ over a set $R$ is a function $A : \Fin{m} \to \Fin{n} \to R$.
\end{Definition}
When talking about matrices from a mathematical point of view, we will write $A_{i j}$ for $A\, i\, j$

%include Code/Algebra/Matrix.lagda

If $R$ is a nonassociative semiring, we can define addition and multiplication of matrices with the usual formulas:
\begin{align*}
  (A + B)_{i j} &= A_{i j} + B_{i j} 
  \intertext{and}
  (A B)_{i j} &= \sum_{k = 1}^n A_{i k} B_{k j} .
\end{align*}
%include /Code/Algebra/MatrixOperations.lagda

The most interesting fact about matrices (to our application) is the following two propositions:
\begin{Proposition}
If $R$ is a a ring (nonassociative semiring), then the matrices of size $n \times n$ over $R$ also form a ring (nonassociative semiring). Additionally, the matrices over $R$ of arbitrary size form an Abelian group (commutative monoid) under matrix addition. In both cases, the zero matrix plays the role of the zero element.
\end{Proposition}
The proof is fairly easy but boring. We provide part of the proof when $R$ is a nonassociative semiring in Agda, is particualar, we prove that addition is commutative, and that the zero matrix is a zero element with multiplication. The whole proof for a nonassociative semiring is available in our library.
%include /Code/Algebra/MatrixProof.lagda
\subsubsection{Upper triangular matrices}
\label{Triangular-matrices}
For our applications, we will be interested in matrices that have no nonzero elements on or below the diagonal.
\begin{Definition}
  A matrix is \emph{upper triangular} if all elements on or below its diagonal are equal to zero.
\end{Definition}
The standard definition of upper triangular matrix allows nonzero elements on the diagonal (for example, the identity matrix is both upper and lower triangular), but we only consider matrices with zeros on the diagonal, so the above definition simplifies our language considerably.
Since we are only interested in upper triangular matrices, we will usually refer to them as just \emph{triangular} matrices.
%include /Code/Algebra/Triangle.lagda

In our library, we use a more general definition of triangularity: a matrix is triangular of degree $d$ if it is zero whenever $j - i \le d$, that is, it is zero on the main diagonal and on $d - 1$ diagonals above it (an upper triangular matrix is triangular of degree $1$). We prove there that if $A$ is triangular of degree $d_A$ and $B$ is triangular of degree $d_B$, then $AB$ is triangular of degree $d_A + d_B$. In particular, any product of at least $n-1$ upper triangular matrices is equal to the zero matrix (there are $n-1$ diagonals above the main diagonal, and the product has to be zero on them all).
