\documentclass{article}
\usepackage{thesis}
\usepackage{ucs}
\include{unicodedefs}

\def\textmu{}
%include agda.fmt

\begin{document}

\title{Algebra of Parallel Programming in Agda -- Text!}
\author{Thomas B\aa\aa th Sj\"{o}blom}
\date{September 4, 2012}
\maketitle
\section{Introduction}
[TODO: Safety]
[TODO: Easier (maybe) to prove correctness along the way]
[TODO: Parallel]
[TODO: Parsing]
\section{Introduction stuff}
\subsection{Agda}
Agda is a dependently typed programming language invented at Chalmers \cite{norell_agda_invented_2007}.
\subsection{Category theory}
Category theory is a theory for unifying mathematics by placing importance on things that are kind of like functions.

There are two reasons for introducing category theory. 
The first is that it will give us a clean and point-free way of reasoning about programs.
The second is that it provides a semantics for the 
[TODO: develop above sentences, AoP works in $\Set$, but what is up with the initial algebra semantics? are lists thought of as sets $\subset \Set$, the set of all lists with elements in $A$ is obviously a set, but is it the best view, what. no wait, nothing important here D:]

The presentation here is based on (the early chapters of) \cite{Awodey} and \cite{MacLane}. The material can also be found in \cite{AoP}, but the authors have reversed the function arrows and writes $f : Y \ot X$ for $f : X \to Y$, something that we feel actually reduces the readability a great deal.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                             CATEGORIES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{Def}
A category $\cat$ is a collection $\obj_{\cat}$ of objects and a collection $\arr_{\cat}$ of arrows $f : X \to Y$, where $X \in \obj_{\cat}$ is the domain of $f$ and $Y \in \obj_{\cat}$ is the codomain of $f$. 
The collections $\obj_{\cat}$ and $\arr_{\cat}$ are required to satisfy the following:
\begin{itemize}
\item For every object $X \in \obj$, there is an identity arrow $\id_X : X \to X$.
\item For every pair $f : A \to B$ and $g : B \to C$ of arrows, there is an arrow $g \comp f : A \to C$
\item For every three arrows $f : A \to B$, $g : B \to C$ and $h : C \to D$, we have $h \comp (g \comp f) = (h \comp g) \comp f$
\item For every arrow $f : A \to B$, we have $f \comp id_A = id_B \comp f = f$. 
\end{itemize}
\end{Def}

These requirements just say that the arrows should behave as functions with compositions and identity functions.

\begin{Ex}
Indeed, the basic example we will work with is the category $\Set$ where the objects are sets and the arrows are functions. This is a category because [TODO: fulfils the axioms].
\end{Ex}

Traditionally in category theory (see for example \cite{Awodey} and \cite{MacLane}), this category is called $\namedcat{Set}$, but we will call it $\Set$ for three main reasons: 
\begin{enumerate}
\item It's the name used in \cite{AoP}.
\item In category theory, the arrows are usually the most importand part of a category.
\item Both it and the next category we will introduce have the same objects.
\end{enumerate}
\begin{Ex}
A second example of a category which we will expand on is the category $\Rel$ where the objects are again sets, but this time, the arrows are relations. 
We write $R : Y \ot X$, where $X$ is the domain and $Y$ the codomain of $R$, following the notation for relations used in \cite{AoPA}. 
Here, composition of $R : Z \ot Y$ and $S : Y \ot X$ is defined as $R \comp S = \{(a, c) \st \exists b : a R b \}$ 
\end{Ex}

Concretely, relations are defined as subsets of $Y \times X$ [TODO or other way around?], but this is a view that we try to avoid with by introducing category theory 

More examples of categories include sets with some structure:
\begin{Ex}
  In the category $\Grp$, the objects are groups and the arrows are group homomorphisms, i.e., functions $\phi$ that respect the group operations: $\phi : \grp{G}{+} \to \grp{H}{\times}$ such that $\phi(x + y) = \phi(x) \times \phi(y)$ and $\phi(-x) = \phi(x)^{-1}$.
\end{Ex}
\begin{Ex}
Other examples along the same lines include for example the category $\Rng$ of rings, and the category $\Mon$ of monoids, in both of which the arrows are the [TODO word] homomorphisms. One can also go further and add other kinds of structure, to get for exampl $\Top$, the category of topological groups, where the objects are topological groups (groups where the operations are continous) [TODO continue]
\end{Ex}
[TODO two more examples: poset and matrixes]
And finally, we present two examples that are quite unlike the previous ones, which present the kind of things that are included in the category definition. Since they are very different from functions, they motivate further specialization of the category definition. We also note that they (like all the categories we have mentioned) have some relation to the later chapters in this thesis.
\begin{Ex}
The first strange example is a so called poset category. Let $X$ be any partially ordered set, that is, a set with a partial order, that is a relation ${}\le{} : Y \ot X$ satisfying the following: 
\begin{enumerate}
\item \label{partial1} For every $x \in X$, $x \le x$.
\item \label{partial2} For every $x$, $y$, if $x \le y$ and $y \le x$, then $x = y$.
\item \label{partial3} For every $x$, $y$, $z$, if $x \le y$ and $y \le z$, then $x \le z$.
\end{enumerate}
Then we get a category if we take as objects elements $x \in X$ and include an arrow $x \to y$ if and only if $x \le y$. By \ref{partial1}, there is an identity arrow for every object. By \ref{partial3}, if there are arrows $x \to y$ and $y \to z$, there is an arrow $x \to z$. Finally, we note that [?]. Hence This satisfies the axioms for a category. Requirement \ref{partial2} guarantees that [?]
\end{Ex}
\begin{Ex}
In our final example, we let the objects be the natural numbers $\N$, and let the arrows be matrixes with coefficients in some (associative) ring. Hence this, too, forms a category.
\end{Ex}
[TODO subcategory, important for defining datatypes in $\Rel$, example of $\Ab$ ]

There are a number of operations that can be performed to create new categories:


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                             FUNCTORS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{Def}
A \emph{functor} $F : \cat[C] \to \cat[D]$ is a pair of mappings:  $F_{\obj} : \obj_{\cat[C]} \to \obj_{\cat[D]}$ and $F_{\arr} : \arr_{\cat[C]} \to \arr_{\cat[D]}$ satisfying
\begin{itemize}
\item If $f : X \to Y$, then $F_{\arr} f : F_{\obj} X \to F_{\obj}$.
\item If $f : X \to Y$ and $g : Y \to Z$, then $F_{\arr} (g \comp f) = (F_{\arr} g) \comp (F_{\arr} f$).
\item The identity arrow $\id_X : X \to X$ is mapped to the identity arrow in $F_{\obj} X$: i.e., $F_{\arr} \id = \id_{F_{\obj}X}$
\end{itemize}
\end{Def}
In what follows, we will refer to both mappings $F_{\obj}$ and $F_{\arr}$ with just $F$.

Some 
\begin{Def}
  A funtor $F : \cat{C} \to \cat{C}$ is called an \emph{endofunctor}.
\end{Def}
[TODO: Examples of functors: product, sum, expand to polynomial functors => regular functors? what are they and are they included in GADT-paper]
[TODO: Natural transformation]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                             ALGEBRAS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

[TODO: Algebra]
\begin{Def}
  An algebra is 
\end{Def}
\subsection{Algebra of Programming}
One point of the introduction of category theory was to allow us to generalize som common list functions to arbitrary datatypes.

An inductive datatype is a 

[TODO: What happens to algebra of programming when using dependent types? There are the papers: \cite{gambino_hyland_2004} and \cite{gadt_semantics_2011}  
\subsection{Parsing}
Parsing is the process of turning a sequence of tokens (for example a string contianing a computer program [TODO]) into a data structure suitable for [something]. It is an important step in compilation. [TODO : need to learn more about basic parsing stuff]

However, in this thesis, we will not focus much on the parsing stuff. This chapter is here mainly as a motivation for what is to come (and so that I will learn some real world stuff!)
[TODO: Example]
\subsubsection{Grammar}
A grammar is a collection of rules, called productions, that 
The productions are made up of terminals and nonterminals.
Additionally, there is a start symbol that specifies [TODO WHAT]
\begin{Def}
  A \emph{grammar} is a tuple $(N, T, S, P)$ where $N$ is the set of nonterminals, $T$ is the set of terminals, $S$ is the start symbol, and $P$ is the set of productions. A \emph{production} is a [TODO thing] sequence of terminals and nonterminals followed by and arrow and another sequence of terminals and nonterminals. [TODO write in symbols] $t_1 \dotsb t_n \to s_1 \dotsb s_m$, $t_i$, $s_i \in N \cup T$
\end{Def}
We will only consider context free grammars. These are grammars 
A language generated by a context free grammar is called a context free language. It is well known that every context free grammar can be turned into a 
\subsubsection{Special kind of grammar -- Chomsky normal form}
It is well known [cite?] that any context free grammar can be turned into one in Chomsky normal form, that is a grammar where [TODO WHAT IS THIS?]. For grammars in Chomsky normal form, it is possible to
\subsection{Transitive Closure}
The result of the parse is the set of all [TODO expand]
\subsection{CYK Algorithm}
[TODO what is it? valiant is related to it, or they wouldn't be on same wikipedia page!]

\subsection{Matrices}
We define a type of abstract matrices, 
\begin{Def}
A matrix $A = (\melt{a}{i}{j})$ is functions $\Fin \to \Fin \to R$
\end{Def}
or, in Agda

\begin{code}
Matrix : ℕ → ℕ → Set
Matrix m n = (i : Fin m) → (j : Fin n) → R
\end{code}

We include also our definitions of addition and multiplication of matrices as examples of how things look.

\begin{code}
-- Matrix addition
_M+_ : ∀ {m n} -> Matrix m n -> Matrix m n -> Matrix m n
_M+_ A B = λ i j → (A i j) R+ (B i j)

-- Matrix multiplication
_M*_ : ∀ {m n p} → Matrix m n → Matrix n p → Matrix m p
A M* B = λ i j → (λ k → A i k) ⊙ (λ k → B k j)
\end{code}
\subsection{Triangular matrices}
One of the main objects we will work with in this thesis is triangular matrices, and we will only ever consider upper triangular matrices with only zeros on the diagonal. For brevity, and because of the similarity with the Agda code we will present soon, we therefore make the following definition.
\begin{Def}
  A matrix $A = (\melt{a}{i}{j})$ is \emph{triangular} if all its entries on or below the diagonal are $0$. That is, $A$ is triagular if $i \ge j$ implies that $\melt{a}{i}{j} = 0$.
\end{Def}
We generalize this to the following.
\begin{Def}
  A matrix $A = (\melt{a}{i}{j})$ is \emph{triangular of degree $d$} if all its entries fewer than $d$ steps above the diagonal are $0$. That is, $A$ is triangular if $d > j - i$ implies that $\melt{a}{i}{j} = 0$.
\end{Def}
\begin{Remark}
  Note that an $n \times n$ matrix that is triangular of degree $d \ge n$ equals the zero matrix. [TODO: Insert agda proof]
\end{Remark}

We prove the following lemma both formally and informally
\begin{Lemma}
  If $A$ is an $m \times n$ matrix that is triangular of degree $d_1$ and $B$ is and $n \times p$ matrix that is triangular of degree $d_2$, then the product $AB$ is an $m \times p$ matrix that is triangular of degree $d_1 + d_2$.
\end{Lemma}
\begin{proof}
  Let $(\melt{c}{i}{j}) = AB$, and let $k$ and $l$ be such that $d_1 + d_2 > k - l$. We have that 
\begin{equation}
  \melt{c}{k}{l} = \sum_{i = 1}^n\melt{a}{k}{i}\melt{b}{i}{l}
\end{equation}
 We want to show that $\melt{c}{k}{l} = 0$.

  
\end{proof}
Heh, triangularity is a homomorphism from matrices with multiplication to $\Z$ with addition
\subsection{Valiants Algorithm}
Valiant's algorithm was initially introduced to show that parsing could be done as quickly as matrix multiplication. It turns out to be an algorithm where most of the work happens in parallel, and this might be useful for developing parallel parsers.

The algorithm takes as input an upper triangular $n \times n$ matrix $A$ (with $0$:s on the diagonal): 
\begin{equation}
A = 
\begin{pmatrix}
0 & a_{1\, 2} & \hdots & a_{1\, n} \\
\vdots & 0 & a_{2\,3} &  \hdots & a_{2\,n}\\
\vdots &
\end{pmatrix}
\end{equation}
and splits the matrix into four parts $\smat{A}{1}{1}$, $\smat{A}{1}{2}$, $\smat{A}{2}{1}$ and $\smat{A}{2}{2}$ along the diagonal: 
\begin{equation}
A = \begin{pmatrix}
  \smat{A}{1}{1} & \smat{A}{1}{2} \\
  \smat{A}{2}{1} & \smat{A}{2}{2}
\end{pmatrix} = \begin{pmatrix}
  \smat{A}{1}{1} & \smat{A}{1}{2} \\
  0 & \smat{A}{2}{2}
\end{pmatrix}
\end{equation}
so that $\smat{A}{1}{1}$ and $\smat{A}{2}{2}$ are both upper triangular

[TODO requirements on the operations, is it enough to have the ring stuff, does $+$ need to be idempotent? (like union) because otherwise, it seems like the algorithm will fail when applied to a non-associative ring (there will be $k\cdot (x_1\cdots x_k)$ instead of $(x_1\cdots x_k)$). So I don't think the algorithm actually works even on non-associative rings, because we can't have inverses for addition (since $x + x = x$ we get that $x = x + (x - x) = (x + x) - x = x - x = 0$ for all $x$, bad. ]

\section{Work}
Our goal is to prove the correctness of Valiant's algorithm, and to do this by presenting a derivation of it from a sensible specification. The first step to do this is to actually formulate the algorithm in Agda, in a way that avoids explicit recursion (using ideas from \cite{AoP}. The next (and final) step is then to find a derivation from the specification to the algorithm. [TODO : related to real programming? not very maybe since we know the final algorithm]
\subsection{Formulation}
The algorithm is made up of two parts, what we refer to as the recursion step and the overlap step. We note that the overlap step is also recursive, and that it is there that the most work is needed to find an appropriate formulation.

We introduce two main recursive datatypes, $\Tri$ and $\SplitMat$. 
The datatype $\Tri$ is esentially a binary tree with a $\SplitMat$ in each splitting, and empty leaves. It is used to represent upper triangular matrixes with zero diagonal and elements from $\R$.
The type $\SplitMat$ on the other hand is essentialy a tree with four subtrees at each splitting (and some extra stuff to make the matrix dimensions fit) and information in the leaves. It is to represent 
. [TODO why the different tree formats?].
In Agda, they are defined by

\begin{code}
data SplitVec : Splitting → Set where
  one : (x : R)   → SplitVec one
  two : ∀ {s₁ s₂} → SplitVec s₁  → SplitVec s₂ → SplitVec (deeper s₁ s₂)

data SplitMat : Splitting → Splitting → Set where
  Sing : (x : R)  → SplitMat one one
  RVec : ∀{s₁ s₂} → SplitVec (deeper s₁ s₂) → SplitMat one (deeper s₁ s₂)
  CVec : ∀{s₁ s₂} → SplitVec (deeper s₁ s₂) → SplitMat (deeper s₁ s₂) one
  quad : ∀{r₁ r₂ c₁ c₂} → SplitMat r₁ c₁ → SplitMat r₁ c₂ → 
                          SplitMat r₂ c₁ → SplitMat r₂ c₂ → 
                          SplitMat (deeper r₁ r₂) (deeper c₁ c₂)

data Tri : Splitting → Set where
  one : Tri one
  two : ∀ {s₁ s₂} → Tri s₁ → SplitMat s₁ s₂ → 
                             Tri s₂ → 
                    Tri (deeper s₁ s₂)
\end{code}

If we allow arbitrary recursion, we can express the algorithm as:

\begin{code}
-- Recursion step:
valiant : ∀ {s} -> Tri s -> Tri s
valiant one = one
valiant (two A C B) = two A' (valiantOverlap A' C B') B'
  where A' = (valiant A)
        B' = (valiant B)

-- Overlap step:
valiantOverlap : ∀ {s₁ s₂} -> Tri s₁ -> SplitMat s₁ s₂ -> Tri s₂ -> SplitMat s₁ s₂
valiantOverlap {one} {one} A' C B' = C
valiantOverlap {one} {deeper s₁ s₂} A' (RVec v) B' = RVec (vectmul v B')
valiantOverlap {deeper s₁ s₂} {one} A' (CVec v) B' = CVec (tvecmul A' v)
valiantOverlap {deeper s₁ s₂} {deeper s₃ s₄} (two A₁ A₂ A₃) (quad C₂ C₄ C₁ C₃) (two B₃ B₂ B₁) = quad X₂ X₄ X₁ X₃
  where X₁ = valiantOverlap A₃ C₁ B₃
        X₂ = valiantOverlap A₁ (splitAdd C₂ (splitMul A₂ X₁)) B₃
        X₃ = valiantOverlap A₃ (splitAdd C₃ (splitMul X₁ B₂)) B₁
        X₄ = valiantOverlap A₁ (splitAdd C₄ (splitAdd (splitMul A₂ X₃) (splitMul X₂ B₂))) B₁
\end{code}

Immediately, it is fairly obvious that the recursion step is simply a catamorphism. That is, we define a function $foldTri$:

\begin{code}
foldTri : ∀ {b} {s : Splitting} {B : Splitting -> Set b} → (one' : B one) → (two' : ∀ {s₁ s₂} -> B s₁ -> SplitMat s₁ s₂ -> B s₂ -> B (deeper s₁ s₂) ) → Tri s → B s
foldTri one' two' one = one'
foldTri one' two' (two T₁ R T₂) = two' (foldTri one' two' T₁) R (foldTri one' two' T₂)
\end{code}

Which allows us to express the recursion step as 

\begin{code}
-- Helper function for overlap
valiantOverlap' : ∀ {s₁ s₂} -> Tri s₁ -> SplitMat s₁ s₂ -> Tri s₂ -> Tri (deeper s₁ s₂)
valiantOverlap' T₁ R T₂ = two T₁ (valiantOverlap T₁ R T₂) T₂

valiantFold : ∀ {s} → Tri s → Tri s
valiantFold = foldTri one valiantOverlap'
\end{code}

The overlap step on the other hand requires quite a bit of work. 
First we note that the result of computing the transitive closure $X_1$ of $C_1$ is required when computing the closures $X_2$ and $X_3$ of $C_2$ and $C_3$ respectively. And that $X_2$ and $X_3$ are required for computing the transitive closure $X_4$ of $C_4$.

\subsubsection{Nested Recursion}
Even if we ignore the problem of sharing -- ideally, we do not want to recompute the value of $X_1$ in the calculation of both $X_2$ and $X_3$, we still have the problem that we need to include a nested recursive call somehow in the algorithm (we want to compute $f (g(f(x)))$ for some particular $f$ and $g$). For the computation of $X_4$ we actually need another level of nested recursion.

This seems similar to course-of-value recursion [thanks Patrik], which has been studied by \cite{histomorphism1}. However, in that case, the components involved are all sub structures of the input structure. Here, the recursion proceeds on elements that are structurally smaller but not sub structures (even though they are shaped like them). Another thing to note is that course-of-value recursion is equivalent to primitive recursion [WIKIPEDIA (only natural numbers)].

Since the recursive calls all happen on smaller structures, we can be sure that the function terminates. Maybe this is something that needs dependent types to work properly?
\subsection{Derivation}
\subsection{Method 1}

\section{Other stuff}
%What about the Floyd Warshall algorithm?
``Bird and Moor'' or ``Bird and de Moor''
\newpage
\bibliography{references}
\bibliographystyle{plainnat}
\end{document}
